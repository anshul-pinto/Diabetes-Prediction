{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=[],[]\n",
    "#reading the file\n",
    "def getdata(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        csvFileReader = csv.reader(csvfile)\n",
    "        next(csvFileReader)\n",
    "        \n",
    "        for row in csvFileReader:\n",
    "            X.append([[float(row[0])],[float(row[1])],[float(row[2])],[float(row[3])],[float(row[4])],[float(row[5])],[float(row[6])],[float(row[7])]])\n",
    "            Y.append(float(row[8]))\n",
    "    \n",
    "    return X,Y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = getdata('diabetes.csv')\n",
    "#have the X and Y ready, time to make training and testing set ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.reshape(X,(768,8))\n",
    "Y=np.reshape(Y,(768,1))\n",
    "\n",
    "#normalizing inputs\n",
    "\n",
    "X_max = (np.max(X, axis=0))\n",
    "\n",
    "for i in range(768):\n",
    "    for j in range(8):\n",
    "        X[i][j]= X[i][j]/X_max[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = X[:700],X[700:],Y[:700],Y[700:]\n",
    "X_train, Y_train = X_train.T,Y_train.T\n",
    "X_test, Y_test = X_test.T,Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 700) (1, 700)\n",
      "(8, 68) (1, 68)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train),np.shape(Y_train))\n",
    "print(np.shape(X_test),np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make all parameters and hyper parameter variables\n",
    "n_1 = 12\n",
    "n_2 = 5\n",
    "n_3 = 5\n",
    "n_4 = 3\n",
    "n_out = 1\n",
    "\n",
    "input_to_nn_X = tf.placeholder(tf.float32, shape=(8,None)) \n",
    "input_to_nn_Y = tf.placeholder(tf.float32, shape=(1,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the neural network\n",
    "#4-Layer neural net\n",
    "def neural_network(x):\n",
    "    tf.set_random_seed(1)\n",
    "    #initializing weight and bias matrices\n",
    "    hidden_layer_1 = {'W1': tf.Variable(tf.random_normal([n_1,8]))*0.01,\n",
    "                      'b1': tf.Variable(tf.zeros([n_1,1]))}\n",
    "    \n",
    "    hidden_layer_2 = {'W2': tf.Variable(tf.random_normal([n_2,n_1]))*0.01,\n",
    "                      'b2': tf.Variable(tf.zeros([n_2,1]))}\n",
    "    \n",
    "    #hidden_layer_3 = {'W3': tf.Variable(tf.random_normal([n_3,n_2]))*0.01,\n",
    "      #                'b3': tf.Variable(tf.zeros([n_3,1]))}\n",
    "    \n",
    "    #hidden_layer_4 = {'W4': tf.Variable(tf.random_normal([n_4,n_3]))*0.01,\n",
    "                      #'b4': tf.Variable(tf.zeros([n_4,1]))}\n",
    "    \n",
    "    output_layer = {'W5': tf.Variable(tf.random_normal([n_out,n_2]))*0.01,\n",
    "                    'b5': tf.Variable(tf.zeros([n_out,1]))}\n",
    "    \n",
    "    #computing neurons' matrix multiplications\n",
    "    #passing through activation functions relu-->relu-->relu-->sigmoid\n",
    "    Z1 = tf.add(tf.matmul(hidden_layer_1['W1'],input_to_nn_X),hidden_layer_1['b1'])\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "   \n",
    "    Z2 = tf.matmul(hidden_layer_2['W2'],A1)+ hidden_layer_2['b2']\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    #Z3 = tf.matmul(hidden_layer_3['W3'],A2)+ hidden_layer_3['b3']\n",
    "    #A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    #Z4 = tf.matmul(hidden_layer_4['W4'],A3)+ hidden_layer_4['b4']\n",
    "    #A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    Z5 = tf.matmul(output_layer['W5'],A2) + output_layer['b5']\n",
    "    A5 = tf.nn.sigmoid(Z5)\n",
    "    \n",
    "    return A5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network(x)\n",
    "    cost = tf.losses.mean_squared_error(input_to_nn_Y,prediction)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.012).minimize(cost)\n",
    "    \n",
    "    epochs = 500000\n",
    "    hm_correct=0\n",
    "    hm_training_correct=0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            _,c = sess.run([optimizer,cost], feed_dict={input_to_nn_X:X_train,\n",
    "                                                  input_to_nn_Y:Y_train})\n",
    "            epoch_loss+=c\n",
    "            if epoch%10000==0:\n",
    "                print('Epoch', epoch+10000,'completed out of ',epochs,' loss:', epoch_loss)\n",
    "            \n",
    "        correct = sess.run(prediction, feed_dict={input_to_nn_X:X_test,\n",
    "                                                  input_to_nn_Y:Y_test})\n",
    "        \n",
    "        #calculating the accuracy of our model for training set and testing set\n",
    "        for i in range(68):\n",
    "            if correct[0][i]<0.5:\n",
    "                correct[0][i]=0\n",
    "            else:\n",
    "                correct[0][i]=1\n",
    "                \n",
    "            if correct[0][i]==Y_test[0][i]:\n",
    "                hm_correct+=1\n",
    "        \n",
    "        training_correct = sess.run(prediction,feed_dict={input_to_nn_X:X_train,\n",
    "                                                  input_to_nn_Y:Y_train})\n",
    "        for i in range(700):\n",
    "            if training_correct[0][i]<0.5:\n",
    "                training_correct[0][i]=0\n",
    "            else:\n",
    "                training_correct[0][i]=1\n",
    "                \n",
    "            if training_correct[0][i]==Y_train[0][i]:\n",
    "                hm_training_correct+=1\n",
    "        print('Training accuracy: ' + str((hm_training_correct/700)*100)+ '%')        \n",
    "        print('Testing accuracy : '+str((hm_correct/68)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000 completed out of  500000  loss: 0.25\n",
      "Epoch 20000 completed out of  500000  loss: 0.15156321227550507\n",
      "Epoch 30000 completed out of  500000  loss: 0.15152208507061005\n",
      "Epoch 40000 completed out of  500000  loss: 0.1515152007341385\n",
      "Epoch 50000 completed out of  500000  loss: 0.1514354944229126\n",
      "Epoch 60000 completed out of  500000  loss: 0.14930619299411774\n",
      "Epoch 70000 completed out of  500000  loss: 0.14739125967025757\n",
      "Epoch 80000 completed out of  500000  loss: 0.14701074361801147\n",
      "Epoch 90000 completed out of  500000  loss: 0.14659591019153595\n",
      "Epoch 100000 completed out of  500000  loss: 0.14613506197929382\n",
      "Epoch 110000 completed out of  500000  loss: 0.1460821032524109\n",
      "Epoch 120000 completed out of  500000  loss: 0.14607302844524384\n",
      "Epoch 130000 completed out of  500000  loss: 0.14607658982276917\n",
      "Epoch 140000 completed out of  500000  loss: 0.14607365429401398\n",
      "Epoch 150000 completed out of  500000  loss: 0.14607253670692444\n",
      "Epoch 160000 completed out of  500000  loss: 0.14607898890972137\n",
      "Epoch 170000 completed out of  500000  loss: 0.14607226848602295\n",
      "Epoch 180000 completed out of  500000  loss: 0.14607001841068268\n",
      "Epoch 190000 completed out of  500000  loss: 0.14607131481170654\n",
      "Epoch 200000 completed out of  500000  loss: 0.1460699588060379\n",
      "Epoch 210000 completed out of  500000  loss: 0.1460823118686676\n",
      "Epoch 220000 completed out of  500000  loss: 0.14607281982898712\n",
      "Epoch 230000 completed out of  500000  loss: 0.14607009291648865\n",
      "Epoch 240000 completed out of  500000  loss: 0.1460716277360916\n",
      "Epoch 250000 completed out of  500000  loss: 0.14606957137584686\n",
      "Epoch 260000 completed out of  500000  loss: 0.14607268571853638\n",
      "Epoch 270000 completed out of  500000  loss: 0.14606952667236328\n",
      "Epoch 280000 completed out of  500000  loss: 0.14607033133506775\n",
      "Epoch 290000 completed out of  500000  loss: 0.1460745930671692\n",
      "Epoch 300000 completed out of  500000  loss: 0.14606903493404388\n",
      "Epoch 310000 completed out of  500000  loss: 0.146079882979393\n",
      "Epoch 320000 completed out of  500000  loss: 0.1460704654455185\n",
      "Epoch 330000 completed out of  500000  loss: 0.146075040102005\n",
      "Epoch 340000 completed out of  500000  loss: 0.14607352018356323\n",
      "Epoch 350000 completed out of  500000  loss: 0.14607036113739014\n",
      "Epoch 360000 completed out of  500000  loss: 0.14607305824756622\n",
      "Epoch 370000 completed out of  500000  loss: 0.14606858789920807\n",
      "Epoch 380000 completed out of  500000  loss: 0.14607074856758118\n",
      "Epoch 390000 completed out of  500000  loss: 0.14606814086437225\n",
      "Epoch 400000 completed out of  500000  loss: 0.1460772007703781\n",
      "Epoch 410000 completed out of  500000  loss: 0.14607353508472443\n",
      "Epoch 420000 completed out of  500000  loss: 0.14606814086437225\n",
      "Epoch 430000 completed out of  500000  loss: 0.14607739448547363\n",
      "Epoch 440000 completed out of  500000  loss: 0.14606766402721405\n",
      "Epoch 450000 completed out of  500000  loss: 0.14606867730617523\n",
      "Epoch 460000 completed out of  500000  loss: 0.14606831967830658\n",
      "Epoch 470000 completed out of  500000  loss: 0.1460692435503006\n",
      "Epoch 480000 completed out of  500000  loss: 0.14606799185276031\n",
      "Epoch 490000 completed out of  500000  loss: 0.14606864750385284\n",
      "Epoch 500000 completed out of  500000  loss: 0.14606782793998718\n",
      "Training accuracy: 78.42857142857143%\n",
      "Testing accuracy : 79.41176470588235%\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(input_to_nn_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
